# Apache Spark
Lecture on Apache Spark for Distributed Systems
## Introduction
This chapter will get you familiar with Apache Spark and how you might use the cluser computing system in in your projects. If you have intrests or projects involving big data, machine learning, and data mining, you might find Spark useful for handling large amounts of data. 
## Definitions 
Cluster
: Multiple computers that perform a task as if it were a single computer.

Open source
: A piece of software that is free for users and allows them to freely distribute the software. The source code is also open to users and they may contribute to the development of the software

Resilient Distributed Datasets (RDD)
: definition
## History and Background
## Architecture
## Open Source
Part of the reason why Spark is widely used is because it is open source. Open source allows companies and users to contribute to the technology. Larger companies also adopt Spark and enhance it based its needs. Spark being open source may be especially helpful for startups that do not have the funds or time to develop a similar product. 

Spark is currently the largest open source big data processing projects and one of the most popular Apache projects. The large and active community ensures Spark is well maintained and well documented. If you are interested in contributing or just reading through the source code, check out Sparkâ€™s Github [repo](https://github.com/apache/spark) and checkout the [guidelines](https://spark.apache.org/contributing.html) provided by Apache. Note that contributing doesn not always mean writing code, so being a part of this project can be simple.  

## Big Data
## Machine Learning 
## Tutorial 
## Sources
